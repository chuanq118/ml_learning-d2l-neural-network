{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1],\n",
      "        [2, 3]])\n",
      "tensor([[0, 1],\n",
      "        [2, 3],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(4).reshape(2, 2)\n",
    "b = torch.arange(6).reshape(3, 2)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 广播机制\n",
    "正常来说,行列不同的矩阵不能相加,但 pytorch 可以任意相同维度的矩阵相加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m r1 \u001b[39m=\u001b[39m a \u001b[39m+\u001b[39;49m b\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(r1)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "r1 = a + b\n",
    "print(r1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 理解错误\n",
    "pytorch 实现不同行列矩阵相加参考了矩阵乘法,那么就需要前一个矩阵的列 = 后一个矩阵的行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "b = b.reshape(2, 3)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m r1 \u001b[39m=\u001b[39m a \u001b[39m+\u001b[39;49m b\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(r1)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "r1 = a + b\n",
    "print(r1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 再次理解错误\n",
    "似乎只支持<strong>一维</strong>不同行列矩阵相加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3]]) tensor([[0, 1, 2]])\n",
      "tensor([[0, 1, 2],\n",
      "        [1, 2, 3],\n",
      "        [2, 3, 4],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(4).reshape(4, 1)\n",
    "b = torch.arange(3).reshape(1, 3)\n",
    "print(a, b)\n",
    "r = a + b\n",
    "print(r)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### view 和 reshape\n",
    "PyTorch 中的 view() 和 reshape() 函数都可以用来重新调整张量的形状。它们的效果是一样的，但是有一些区别12。  \n",
    "- view() 方法只能改变连续的 (contiguous) 张量，否则需要先调用 .contiguous() 方法；而 .reshape() 方法不受此限制；如果对 tensor 调用过 transpose, permute 等操作的话会使该 tensor 在内存中变得不再连续。\n",
    "- .view() 方法返回的张量总是和原来的张量共享一份相同的数据，而 .reshape() 在新形状满足一定条件时会共享相同一份数据，否则会复制一份新的数据。\n",
    "- 两者对于原始张量的连续性要求不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m a \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(\u001b[39m1\u001b[39m \u001b[39m<<\u001b[39m \u001b[39m3\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msource a array: \u001b[39m\u001b[39m{\u001b[39;00ma\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m c \u001b[39m=\u001b[39m a\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m \u001b[39m<<\u001b[39m \u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "a = torch.arange(1 << 3)\n",
    "print(f\"source a array: {a}\")\n",
    "c = a.view(1 << 2, 2)\n",
    "c[:] = 2\n",
    "print(f\"after view: {a}\")\n",
    "b = a.reshape(1 << 2, 2)\n",
    "b[:] = 2\n",
    "print(f\"after reshape: {a}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1],\n",
       "         [2, 3]],\n",
       "\n",
       "        [[4, 5],\n",
       "         [6, 7]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(1 << 3).reshape(2, 2, 2)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4,  6],\n",
      "        [ 8, 10]])\n",
      "tensor([[ 2,  4],\n",
      "        [10, 12]])\n"
     ]
    }
   ],
   "source": [
    "print(a.sum(axis=0))\n",
    "print(a.sum(axis=1))\n",
    "# 第二个维度求和就是 将第二个维度里面的矩阵进行相加以去除第二个维度 例如 0 + 2, 1 + 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我觉得是分子布局，X和W内积是标量，<X,W>(标量)对W(列向量)求偏导=X转置(行向量)   符合分子布局"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l-zh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b320c748c085e173cc38d9117a1564438f46235551c4648d4648ac8881850d65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
